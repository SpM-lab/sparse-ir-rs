= Sparse sampling notes (IR + Matsubara): selecting points via the U-matrix

This note describes a practical sampling strategy for IR-based fitting that is based on the *IR basis matrix* (the “U-matrix”), rather than directly sampling a kernel matrix.

Core idea:

- To recover IR coefficients, we solve a least-squares problem whose design matrix is built from IR basis functions evaluated at sampling points.
- Therefore “choosing sampling points” is naturally formulated as *selecting rows of the design matrix*.
- Symmetry of the IR basis (centrosymmetry in imaginary time; ± symmetry in Matsubara) can be enforced by construction.
- Candidate points may be generated by heuristics (kernel hints + Gauss–Legendre segments), but the scheme works for *any* user-provided candidate points.
- For nonuniform candidates, a natural weight is given by a nearest-neighbor (Voronoi) partition of the domain.

== 1. IR fitting and the U-matrix

Let $t = tau / beta in [0, 1]$.
Fix the IR truncation size $N_l$.
Let $U_l(t)$ be the IR basis functions.

We approximate

$ G(t) approx sum_(l=0)^(N_l-1) g_l U_l(t). $

Given sampling points $t_j$ (for $j=1..k$), define the design matrix

$ A_(j,l) = U_l(t_j), $

so that

$ A g approx y, \; y_j = G(t_j). $

A stable sampling set is one for which $A$ has full column rank and a moderate condition number.

=== 1.1 Row selection viewpoint

Each sampling point $t_j$ corresponds to a *row* of $A$.
So selecting sampling points is selecting a subset of rows.

Many algorithms are naturally phrased as column selection (e.g. CPQR), so a common trick is:

- row selection for $A$ is column selection for $A^T$.

Concretely, if you build a candidate matrix $A_("cand")$ from all candidate points, then applying CPQR to $A_("cand")^T$ yields a pivot order of candidate rows.

== 2. Centrosymmetry in imaginary time (mirror pairing)

IR basis functions satisfy the mirror-sign relation

$ U_l(1 - t) = (-1)^l U_l(t). $

Let the mirror map be $m(t) = 1 - t$.
Then sampling points naturally come in mirror pairs $(t, 1-t)$.

=== 2.1 Even/odd decoupling by sum/difference

Define

- $G_+(t) = G(t) + G(1-t)$,
- $G_-(t) = G(t) - G(1-t)$.

Using the mirror-sign relation, we obtain

$ G_+(t) approx 2 sum_(l in E) g_l U_l(t), $

$ G_-(t) approx 2 sum_(l in O) g_l U_l(t), $

where `E` and `O` are the even/odd IR indices in `0..N_l-1`.
So the fitting splits into two independent subproblems (even-$l$ and odd-$l$).

=== 2.2 Why representatives are about half (dimension count)

If we enforce mirror pairing, the total number of sampled points is approximately

$ k approx 2 k_("rep"), $

where $k_("rep")$ is the number of representatives in (say) $t in [0, 1/2]$.

To determine $N_l$ coefficients we need at least $k >= N_l$ independent equations.
Under pairing this implies

$ 2 k_("rep") >= N_l \;⇒\; k_("rep") >= ceil(N_l / 2). $

More precisely, the two decoupled blocks have sizes $N_("even")$ and $N_("odd")$, and we need

$ k_("rep") >= max(N_("even"), N_("odd")) approx N_l/2. $

This explains why selecting about half as many representatives is the natural baseline.

== 3. Candidate points and natural weights on [0,1]

=== 3.1 Candidate generation (recommended, but not required)

A good way to produce candidates is a heuristic mesh such as:

- kernel hints (regions where basis functions vary rapidly), plus
- piecewise Gauss–Legendre nodes within segments.

However, the selection scheme does *not* depend on this choice.

=== 3.2 General input: any candidate points are acceptable

If the user provides an arbitrary candidate set (possibly nonuniform or adaptive), we can still proceed.
The only requirements are:

- points lie within the domain (e.g. $[0,1]$),
- points are distinct after deduplication,
- and we can evaluate $U_l(t)$ on them.

=== 3.3 Natural weights from a nearest-neighbor partition

Given any sorted candidate set

$ 0 <= t_0 < t_1 < ... < t_(N-1) <= 1, $

a natural partition is the nearest-neighbor (Voronoi) partition on $[0,1]$.
The length of each cell gives a natural weight.

In 1D this yields

- interior: $w_i = (t_(i+1) - t_(i-1)) / 2$,
- endpoints: $w_0 = t_1 - t_0$, $w_(N-1) = t_(N-1) - t_(N-2)$.

These weights satisfy $sum_i w_i approx 1$.

=== 3.4 Using weights in point selection

Weights can be incorporated by scaling rows of the design matrix.
If $A_(j,l) = U_l(t_j)$ and $w_j$ are weights, define

$ A_w_(j,l) = sqrt(w_j) A_(j,l). $

Then selecting points to improve the conditioning of $A_w$ corresponds to a weighted inner product on the domain.

== 4. Monitoring conditioning while adding points

When you add sampling points, you add rows to $A$ (or $A_w$).
Let $A in RR^(k times N_l)$ be the current matrix, and let $a^T$ be the new row.
Form $A'$ by appending $a^T$.

Define the Gram matrix $G = A^T A in RR^(N_l times N_l)$.
Then

$ G' = A'^T A' = G + a a^T. $

Since $a a^T$ is positive semidefinite, for every vector $x$,

$ x^T G' x >= x^T G x. $

Consequently:

- eigenvalues of $G$ are nondecreasing as points are added,
- singular values of $A$ are nondecreasing as points are added.

For 2-norm condition numbers,

$ kappa_2(G) = kappa_2(A)^2, \; kappa_2(A) = sqrt(kappa_2(G)). $

Note: while $sigma_min(A)$ and $sigma_max(A)$ are monotone in the number of points, $kappa_2(A)$ is not guaranteed to decrease monotonically.

If the basis is orthonormal in the target weighted inner product, then the ideal continuous Gram is the identity.
With sufficiently dense candidates and consistent weights, one expects $G$ to approach $I$, and thus $kappa_2(G)$ to approach 1.

== 5. Matsubara domain: selecting points via the U-hat matrix

On the Matsubara axis we similarly fit coefficients using a feature/design matrix built from the Matsubara-evaluated basis.
Let $nu_n$ be Matsubara frequencies.
Define a feature matrix using $U_("hat")_l(i nu_n)$ evaluated at candidate indices.

The symmetry is $nu -> -nu$.
For many physical objects one has the conjugacy relation

$ G(-i nu) = overline(G(i nu)). $

So it is natural to select only positive representatives and then add their negatives.

=== 5.1 Avoiding complex arithmetic by real/imag stacking

To avoid complex arithmetic in the selection step, build a real matrix by stacking real and imaginary parts.
For a complex candidate matrix $F(i nu_n)$, define

- `F_even = Re(F)`,
- `F_odd  = Im(F)`,
- `A = vcat(alpha * F_even, beta * F_odd)`.

Then run row selection (via column pivots of $A^T$) on this real stacked matrix.

=== 5.2 Dimensionless variable and log-like candidate generation

Let $Lambda = beta omega_("max")$ and define the dimensionless frequency

$ u_n = nu_n / omega_("max"). $

Candidates are still integer indices $n$, but it is convenient to generate them using a log-like grid in $u$:

- choose target magnitudes in $u$ on a log grid,
- map each target to the nearest integer $n$,
- deduplicate indices,
- mirror to obtain ± pairs if desired.

=== 5.3 Voronoi-style weights for nonuniform Matsubara candidates

Given a sorted positive candidate set

$ 0 <= u_0 < u_1 < ... < u_(N-1), $

define cell-width weights

- interior: $w_i = (u_(i+1) - u_(i-1)) / 2$,
- endpoints: $w_0 = u_1 - u_0$, $w_(N-1) = u_(N-1) - u_(N-2)$.

If you expand to ± pairs, use the same weight for $+u_i$ and $-u_i$.
For a fixed point (bosonic $nu=0$), treat it as its own mirror and handle carefully if any weight construction would make it vanish.

== 6. Oversampling → refine candidates → reselect (practical improvement)

A useful practical improvement loop is:

1. start from an initial candidate set (e.g. hint + Gauss–Legendre segments),
2. select a sampling set,
3. increase the candidate density using the selected set,
4. rebuild weights,
5. reselect.

On a continuous domain, refinement can be done by subdividing intervals around selected points.
In Matsubara, refinement can be done by inserting extra targets between neighboring selected $u$ values and mapping back to integer indices.

== 7. Non-orthogonal bases: whitening (not unitary)

If your basis is not orthogonal in the target inner product, you can still apply the same selection ideas.
But the transformation that orthogonalizes the basis is typically not unitary.

Let $G_0$ be the Gram matrix under the target inner product.
If $G_0 = L L^T$ (Cholesky), then a whitened design matrix is

$ A_w = A L^(-T). $

Unlike a unitary rotation, this changes singular values and can improve conditioning by removing correlations between basis columns.

== 8. About singular values from QR / CPQR (rank-revealing heuristic)

For a thin QR factorization $A = Q R$ with $Q$ orthonormal, $A$ and $R$ have the same singular values.
However, the diagonal entries $|R_(i i)|$ are not singular values.

Pivoted QR (CPQR / RRQR) is often used as a heuristic rank-revealing procedure: the decay of $|R_(i i)|$ can indicate an effective numerical rank cutoff.
